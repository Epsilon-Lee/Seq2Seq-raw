{
  "training": {
    "optimizer": "adam",
    "grad_threshold": 5.0,
    "lr": 0.0001,
    "max_epoch": 30,
    "train_batch_size": 64,
    "dev_batch_size": 32,
    "test_batch_size": 32
  },
  "evaluation": {
    "max_decode_len": 60
  },
  "management": {
    "logfile_path": "../Logs/Seq2Seq-raw/train_modular_globalatt.log",
    "logging_interval": 32,
    "print_samples": 320,
    "print_number": 8,
    "eval_interval": 2000,
    "gpuid": 0
  },
  "data": {
    "train_path": "./IWSLT/trainDataset.pt",
    "dev_path": "./IWSLT/devDataset.pt",
    "test_path": "./IWSLT/testDataset.pt",
    "src_dict_path": "./IWSLT/de.30k.dict",
    "tgt_dict_path": "./IWSLT/en.30k.dict",
    "save_dir": "../Models/Seq2Seq-raw/",
    "load_dir": ""
  },
  "model": {
    "encoder": {
      "emb_size": 256,
      "hid_size": 512,
      "bidirectional": 1,
      "rnn_cell_type": "gru",
      "is_packed": 1,
      "batch_first": 1,
      "num_layers": 2,
      "dropout": 0.3
    },
    "decoder": {
      "emb_size": 256,
      "hid_size": 512,
      "rnn_cell_type": "gru",
      "num_layers": 2,
      "dropout": 0,
      "global_attention_type": "concat"
    },
    "generator": {
      "dim_lst": [512],
      "num_layers": 1
    }
  }
}
